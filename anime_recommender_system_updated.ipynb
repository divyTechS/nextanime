{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8191288d-e243-4237-ae43-438f70b4fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 20/20 [00:32<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for 'Naruto':\n",
      "\n",
      "Road to Ninja: Naruto the Movie (Score: 0.39)\n",
      "Naruto: Shippuden (Score: 0.31)\n",
      "Boruto: Naruto Next Generations (Score: 0.29)\n",
      "Boruto: Naruto the Movie (Score: 0.19)\n",
      "The Last: Naruto the Movie (Score: 0.13)\n",
      "Solo Leveling Season 2 -Arise from the Shadow- (Score: 0.12)\n",
      "Fruits Basket (2019) (Score: 0.10)\n",
      "Gurren Lagann (Score: 0.10)\n",
      "Ninja Kamui (Score: 0.10)\n",
      "Demon Slayer -Kimetsu no Yaiba- The Movie: Mugen Train (Score: 0.09)\n"
     ]
    }
   ],
   "source": [
    "# Anime Recommender System using AniList API\n",
    "# This notebook builds a content-based anime recommender system using data from the AniList GraphQL API.\n",
    "\n",
    "!pip install -q requests beautifulsoup4 fuzzywuzzy python-Levenshtein\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import process\n",
    "tqdm.pandas()\n",
    "\n",
    "def clean_description(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def fetch_anime_page(page=1, per_page=50):\n",
    "    url = 'https://graphql.anilist.co'\n",
    "    query = '''\n",
    "    query ($page: Int, $perPage: Int) {\n",
    "      Page(page: $page, perPage: $perPage) {\n",
    "        pageInfo {\n",
    "          total\n",
    "          currentPage\n",
    "          lastPage\n",
    "          hasNextPage\n",
    "        }\n",
    "        media(type: ANIME, sort: POPULARITY_DESC) {\n",
    "          id\n",
    "          title {\n",
    "            romaji\n",
    "            english\n",
    "            native\n",
    "          }\n",
    "          trailer {\n",
    "            id\n",
    "            site\n",
    "          }\n",
    "          description(asHtml: false)\n",
    "          genres\n",
    "          tags {\n",
    "            name\n",
    "            rank\n",
    "            isGeneralSpoiler\n",
    "          }\n",
    "          averageScore\n",
    "          popularity\n",
    "          episodes\n",
    "          duration\n",
    "          startDate {\n",
    "            year\n",
    "            month\n",
    "            day\n",
    "          }\n",
    "          coverImage {\n",
    "            large\n",
    "          }\n",
    "          studios(isMain: true) {\n",
    "            nodes {\n",
    "              name\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    variables = {'page': page, 'perPage': per_page}\n",
    "    try:\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page {page}: {e}\")\n",
    "        return {}\n",
    "\n",
    "anime_list = []\n",
    "max_pages = 20\n",
    "\n",
    "for page in tqdm(range(1, max_pages + 1)):\n",
    "    data = fetch_anime_page(page)\n",
    "    if 'data' not in data:\n",
    "        break\n",
    "    media = data['data']['Page']['media']\n",
    "    for anime in media:\n",
    "        anime_list.append({\n",
    "            'id': anime['id'],\n",
    "            'title': anime['title']['english'] or anime['title']['romaji'],\n",
    "            'romaji': anime['title']['romaji'],\n",
    "            'native': anime['title']['native'],\n",
    "            'description': clean_description(anime['description']),\n",
    "            'genres': ', '.join(anime['genres']),\n",
    "            'tags': ', '.join([tag['name'] for tag in anime['tags'] if not tag['isGeneralSpoiler']]),\n",
    "            'average_score': anime['averageScore'],\n",
    "            'popularity': anime['popularity'],\n",
    "            'episodes': anime['episodes'],\n",
    "            'duration': anime['duration'],\n",
    "            'start_date': f\"{anime['startDate']['year']}-{anime['startDate']['month'] or 1}-{anime['startDate']['day'] or 1}\",\n",
    "            'trailer_url': f\"https://www.youtube.com/watch?v={anime['trailer']['id']}\" if anime.get('trailer') and anime['trailer'].get('site') == 'youtube' else None,\n",
    "            'cover_image': anime['coverImage']['large'],\n",
    "            'studios': ', '.join([studio['name'] for studio in anime['studios']['nodes']]),\n",
    "            'anilist_url': f\"https://anilist.co/anime/{anime['id']}\"\n",
    "        })\n",
    "\n",
    "anime_df = pd.DataFrame(anime_list)\n",
    "anime_df.dropna(subset=['title'], inplace=True)\n",
    "anime_df.head()\n",
    "\n",
    "anime_df['combined_features'] = anime_df['genres'] + ' ' + anime_df['tags'] + ' ' + anime_df['description'].fillna('')\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(anime_df['combined_features'])\n",
    "similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "def recommend_anime(title, top_n=10):\n",
    "    title = title.lower()\n",
    "    titles = anime_df['title'].str.lower().tolist()\n",
    "    best_match = process.extractOne(title, titles)\n",
    "    if best_match[1] < 80:\n",
    "        print(f\"Anime '{title}' not found. Did you mean '{best_match[0]}'?\")\n",
    "        return\n",
    "    idx = anime_df[anime_df['title'].str.lower() == best_match[0]].index[0]\n",
    "    sim_scores = list(enumerate(similarity[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    print(f\"Top {top_n} recommendations for '{anime_df.loc[idx, 'title']}':\\n\")\n",
    "    for i, score in sim_scores:\n",
    "        print(f\"{anime_df.loc[i, 'title']} (Score: {score:.2f})\")\n",
    "\n",
    "# Example\n",
    "recommend_anime(\"Naruto\")\n",
    "\n",
    "import pickle\n",
    "with open(\"anime_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(anime_df, f)\n",
    "with open(\"anime_similarity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(similarity, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b664fd-3e37-478d-8965-5555dba771b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'romaji', 'native', 'description', 'genres', 'tags',\n",
      "       'average_score', 'popularity', 'episodes', 'duration', 'start_date',\n",
      "       'trailer_url', 'cover_image', 'studios', 'anilist_url',\n",
      "       'combined_features'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(anime_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8e7762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 20/20 [00:33<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def clean_description(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def fetch_anime_page(page=1, per_page=50):\n",
    "    url = 'https://graphql.anilist.co'\n",
    "    query = '''\n",
    "    query ($page: Int, $perPage: Int) {\n",
    "      Page(page: $page, perPage: $perPage) {\n",
    "        pageInfo {\n",
    "          total\n",
    "          currentPage\n",
    "          lastPage\n",
    "          hasNextPage\n",
    "        }\n",
    "        media(type: ANIME, sort: POPULARITY_DESC) {\n",
    "          id\n",
    "          title {\n",
    "            romaji\n",
    "            english\n",
    "            native\n",
    "          }\n",
    "          trailer {\n",
    "            id\n",
    "            site\n",
    "          }\n",
    "          description(asHtml: false)\n",
    "          genres\n",
    "          tags {\n",
    "            name\n",
    "            isGeneralSpoiler\n",
    "          }\n",
    "          averageScore\n",
    "          popularity\n",
    "          episodes\n",
    "          duration\n",
    "          startDate {\n",
    "            year\n",
    "            month\n",
    "            day\n",
    "          }\n",
    "          coverImage {\n",
    "            large\n",
    "          }\n",
    "          studios(isMain: true) {\n",
    "            nodes {\n",
    "              name\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    variables = {'page': page, 'perPage': per_page}\n",
    "    try:\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page {page}: {e}\")\n",
    "        return {}\n",
    "\n",
    "anime_list = []\n",
    "max_pages = 20\n",
    "for page in tqdm(range(1, max_pages + 1)):\n",
    "    data = fetch_anime_page(page)\n",
    "    if 'data' not in data:\n",
    "        break\n",
    "    media = data['data']['Page']['media']\n",
    "    for anime in media:\n",
    "        title = anime['title'].get('english') or anime['title'].get('romaji') or anime['title'].get('native')\n",
    "        if not title:\n",
    "            continue\n",
    "        anime_list.append({\n",
    "            'id': anime['id'],\n",
    "            'title': title,\n",
    "            'romaji': anime['title']['romaji'],\n",
    "            'native': anime['title']['native'],\n",
    "            'description': clean_description(anime.get('description', '')),\n",
    "            'genres': ', '.join(anime.get('genres', [])),\n",
    "            'tags': ', '.join([tag['name'] for tag in anime.get('tags', []) if not tag.get('isGeneralSpoiler')]),\n",
    "            'average_score': anime.get('averageScore'),\n",
    "            'popularity': anime.get('popularity'),\n",
    "            'episodes': anime.get('episodes'),\n",
    "            'duration': anime.get('duration'),\n",
    "            'start_date': f\"{anime['startDate'].get('year', '')}-{anime['startDate'].get('month', 1)}-{anime['startDate'].get('day', 1)}\",\n",
    "            'trailer_url': f\"https://www.youtube.com/watch?v={anime['trailer']['id']}\" if anime.get('trailer') and anime['trailer'].get('site') == 'youtube' else None,\n",
    "            'cover_image': anime['coverImage']['large'] if anime.get('coverImage') else None,\n",
    "            'studios': ', '.join([studio['name'] for studio in anime.get('studios', {}).get('nodes', [])]),\n",
    "            'anilist_url': f\"https://anilist.co/anime/{anime['id']}\"\n",
    "        })\n",
    "\n",
    "anime_df = pd.DataFrame(anime_list)\n",
    "anime_df.dropna(subset=['title'], inplace=True)\n",
    "anime_df['combined_features'] = anime_df['genres'] + ' ' + anime_df['tags'] + ' ' + anime_df['description'].fillna('')\n",
    "anime_df['title_lower'] = anime_df['title'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f71bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(anime_df['combined_features'])\n",
    "similarity = cosine_similarity(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e769c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend_anime(title, top_n=10, weight_popularity=0.3):\n",
    "    title = title.lower()\n",
    "    titles = anime_df['title_lower'].tolist()\n",
    "    best_match = process.extractOne(title, titles)\n",
    "    if best_match[1] < 80:\n",
    "        print(f\"Anime '{title}' not found. Did you mean '{best_match[0]}'?\")\n",
    "        return\n",
    "    idx = anime_df[anime_df['title_lower'] == best_match[0]].index[0]\n",
    "    sim_scores = list(enumerate(similarity[idx]))\n",
    "    weighted_scores = []\n",
    "    for i, sim in sim_scores:\n",
    "        pop_score = anime_df.loc[i, 'popularity'] or 0\n",
    "        pop_score_norm = pop_score / anime_df['popularity'].max()\n",
    "        score = sim * (1 - weight_popularity) + pop_score_norm * weight_popularity\n",
    "        weighted_scores.append((i, score))\n",
    "    sorted_scores = sorted(weighted_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    print(f\"Top {top_n} recommendations for '{anime_df.loc[idx, 'title']}':\\n\")\n",
    "    for i, score in sorted_scores:\n",
    "        print(f\"{anime_df.loc[i, 'title']} (Score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401544b8-7a04-4e40-bad8-d990ae4c711b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
